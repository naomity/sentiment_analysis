{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Sentiment Classification "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import textblob\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first import text files from train & test folders into a dataframe, on which data cleaning is done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = 'aclImdb/train/'\n",
    "test_path = 'aclImdb/test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 3)\n"
     ]
    }
   ],
   "source": [
    "#import training data into a dataframe\n",
    "indices = []\n",
    "text = []\n",
    "rating = []\n",
    "i = 0\n",
    "for filename in os.listdir(train_path+'pos'):\n",
    "    data = open(train_path+'pos/'+filename, 'r' , encoding='ISO-8859-1').read()\n",
    "    indices.append(i)\n",
    "    text.append(data)\n",
    "    rating.append('1')\n",
    "    i = i + 1\n",
    "for filename in os.listdir(train_path+'neg'):\n",
    "    data = open(train_path+'neg/'+filename, 'r' , encoding='ISO-8859-1').read()\n",
    "    indices.append(i)\n",
    "    text.append(data)\n",
    "    rating.append('0')\n",
    "    i = i + 1\n",
    "Dataset = list(zip(indices,text,rating))\n",
    "df = pd.DataFrame(data = Dataset, columns=['row_Number', 'review', 'sentiment'])\n",
    "\n",
    "#print(df.head())\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import test data\n",
    "indices = []\n",
    "text = []\n",
    "rating = []\n",
    "i = 0\n",
    "for filename in os.listdir (test_path+'pos'):\n",
    "    data = open(test_path+'pos/'+filename,'r',encoding='ISO-8859-1').read()\n",
    "    indices.append(i)\n",
    "    text.append(data)\n",
    "    rating.append('1')\n",
    "    i = i + 1\n",
    "for filename in os.listdir(test_path+'neg'):\n",
    "    data = open(test_path+'neg/'+filename, 'r' , encoding='ISO-8859-1').read()\n",
    "    indices.append(i)\n",
    "    text.append(data)\n",
    "    rating.append('0')\n",
    "    i = i + 1\n",
    "Dataset = list(zip(indices,text,rating))\n",
    "dftest = pd.DataFrame(data = Dataset, columns=['row_Number', 'review', 'sentiment'])\n",
    "\n",
    "#print(dftest.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data cleaning on training data --can be more concise\n",
    "\n",
    "df['review'] = df['review'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "df['review'] = df['review'].map(lambda x: re.sub(r'([^\\s\\w]|_)+', '', x))\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "df['review'] = df['review'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "from textblob import Word\n",
    "df['review'] = df['review'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n",
    "freq = pd.Series(' '.join(df['review']).split()).value_counts()[:10] #print(freq) to check the words\n",
    "df['review'] = df['review'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\n",
    "#freq = pd.Series(' '.join(df['review']).split()).value_counts()[-80:] since we are only looking at most freq ones, this is unnecessary\n",
    "#df['review'] = df['review'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data cleaning on testing data\n",
    "\n",
    "dftest['review'] = dftest['review'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "dftest['review'] = dftest['review'].map(lambda x: re.sub(r'([^\\s\\w]|_)+', '', x))\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "dftest['review'] = dftest['review'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n",
    "from textblob import Word\n",
    "dftest['review'] = dftest['review'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n",
    "freq = pd.Series(' '.join(dftest['review']).split()).value_counts()[:10] #print(freq) to check the words\n",
    "dftest['review'] = dftest['review'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\n",
    "#freq = pd.Series(' '.join(dftest['review']).split()).value_counts()[-80:] \n",
    "#dftest['review'] = dftest['review'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After cleaning, we will create corresponding data for train, validation (will be denoted as 'val') and test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Naomi\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#split df into train and validation data\n",
    "x = df.review\n",
    "y = df.sentiment\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "SEED = 2000\n",
    "x_train,x_val_and_test,y_train,y_val_and_test = train_test_split(x,y,train_size=0.9,random_state=SEED)\n",
    "x_val,x_test,y_val,y_test = train_test_split(x_val_and_test,y_val_and_test,test_size=0) #test data is provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define test data as provided\n",
    "x_test = dftest.review\n",
    "y_test = dftest.sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Word2Vec models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this process, two Word2Vec models were trained using Continuous Bag of Words and Skip Gram models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Naomi\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "import gensim\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "import multiprocessing\n",
    "from sklearn import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label the reviews for training\n",
    "def labelize_review_ug(review,label):\n",
    "    result = []\n",
    "    prefix = label\n",
    "    for i, t in zip(review.index, review):\n",
    "        result.append(TaggedDocument(t.split(), [prefix + '_%s' % i]))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_x = pd.concat([x_train,x_val,x_test])\n",
    "all_x_w2v = labelize_review_ug(all_x, 'all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 50000/50000 [00:00<00:00, 1564501.74it/s]\n"
     ]
    }
   ],
   "source": [
    "cores = multiprocessing.cpu_count()\n",
    "model_ug_cbow = Word2Vec(sg=0, size=100, negative=5, window=2, min_count=2, workers=cores, alpha=0.065, min_alpha=0.065)\n",
    "model_ug_cbow.build_vocab([x.words for x in tqdm(all_x_w2v)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training word2vec using both methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 50000/50000 [00:00<00:00, 1564536.75it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 50000/50000 [00:00<00:00, 1788051.53it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 50000/50000 [00:00<00:00, 1788112.51it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 50000/50000 [00:00<00:00, 1788036.29it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 50000/50000 [00:00<00:00, 1788082.02it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 50000/50000 [00:00<00:00, 1564595.11it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 50000/50000 [00:00<00:00, 1564513.41it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 50000/50000 [00:00<00:00, 1788234.49it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 50000/50000 [00:00<00:00, 1454850.19it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 50000/50000 [00:00<00:00, 1137919.77it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 50000/50000 [00:00<00:00, 1137839.51it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 50000/50000 [00:00<00:00, 1787899.09it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 50000/50000 [00:00<00:00, 1564630.13it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 50000/50000 [00:00<00:00, 1564618.46it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 50000/50000 [00:00<00:00, 1564688.50it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 50000/50000 [00:00<00:00, 1788204.00it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 50000/50000 [00:00<00:00, 1564490.07it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 50000/50000 [00:00<00:00, 1788066.78it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 50000/50000 [00:00<00:00, 1788112.51it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 50000/50000 [00:00<00:00, 1788951.45it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 50000/50000 [00:00<00:00, 1564606.79it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 50000/50000 [00:00<00:00, 1390841.14it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 50000/50000 [00:00<00:00, 1390721.24it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 50000/50000 [00:00<00:00, 2086054.19it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 50000/50000 [00:00<00:00, 1788066.78it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 50000/50000 [00:00<00:00, 1390785.80it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 50000/50000 [00:00<00:00, 1787929.58it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 50000/50000 [00:00<00:00, 2085991.94it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 50000/50000 [00:00<00:00, 1137864.20it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 50000/50000 [00:00<00:00, 1787822.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for epoch in range(30):\n",
    "    model_ug_cbow.train(utils.shuffle([x.words for x in tqdm(all_x_w2v)]), total_examples=len(all_x_w2v), epochs=1)\n",
    "    model_ug_cbow.alpha -= 0.002\n",
    "    model_ug_cbow.min_alpha = model_ug_cbow.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 50000/50000 [00:00<00:00, 1251687.30it/s]\n"
     ]
    }
   ],
   "source": [
    "model_ug_sg = Word2Vec(sg=1, size=100, negative=5, window=2, min_count=2, workers=cores, alpha=0.065, min_alpha=0.065)\n",
    "model_ug_sg.build_vocab([x.words for x in tqdm(all_x_w2v)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████| 50000/50000 [00:00<00:00, 1564245.01it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 50000/50000 [00:00<00:00, 1787975.31it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 50000/50000 [00:00<00:00, 1788143.01it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 50000/50000 [00:00<00:00, 1564606.79it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 50000/50000 [00:00<00:00, 1564676.83it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 50000/50000 [00:00<00:00, 1788112.51it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 50000/50000 [00:00<00:00, 1564688.50it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 50000/50000 [00:00<00:00, 1564630.13it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 50000/50000 [00:00<00:00, 1787853.37it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 50000/50000 [00:00<00:00, 1390730.46it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 50000/50000 [00:00<00:00, 1788036.29it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 50000/50000 [00:00<00:00, 1251679.83it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 50000/50000 [00:00<00:00, 1251597.66it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 50000/50000 [00:00<00:00, 2086074.94it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 50000/50000 [00:00<00:00, 1785676.46it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 50000/50000 [00:00<00:00, 1564513.41it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 50000/50000 [00:00<00:00, 1785691.66it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 50000/50000 [00:00<00:00, 1564711.85it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 50000/50000 [00:00<00:00, 1564525.08it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 50000/50000 [00:00<00:00, 1788036.29it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 50000/50000 [00:00<00:00, 1390758.13it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 50000/50000 [00:00<00:00, 1788082.02it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 50000/50000 [00:00<00:00, 1564688.50it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 50000/50000 [00:00<00:00, 1787944.82it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 50000/50000 [00:00<00:00, 1788051.53it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 50000/50000 [00:00<00:00, 1564665.15it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 50000/50000 [00:00<00:00, 1788112.51it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 50000/50000 [00:00<00:00, 1787838.13it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 50000/50000 [00:00<00:00, 1390730.46it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 50000/50000 [00:00<00:00, 1564513.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for epoch in range(30):\n",
    "    model_ug_sg.train(utils.shuffle([x.words for x in tqdm(all_x_w2v)]), total_examples=len(all_x_w2v), epochs=1)\n",
    "    model_ug_sg.alpha -= 0.002\n",
    "    model_ug_sg.min_alpha = model_ug_sg.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ug_cbow.save('w2v_model_ug_cbow.word2vec')\n",
    "model_ug_sg.save('w2v_model_ug_sg.word2vec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparation for CNN\n",
    "Then we start to prepare for CNN traing, first import word2vec using gensim library and concatenate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "model_ug_cbow = KeyedVectors.load('w2v_model_ug_cbow.word2vec')\n",
    "model_ug_sg = KeyedVectors.load('w2v_model_ug_sg.word2vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 73744 word vectors.\n"
     ]
    }
   ],
   "source": [
    "#concatenateing vectors of 2 models\n",
    "embeddings_index = {}\n",
    "for w in model_ug_cbow.wv.vocab.keys():\n",
    "    embeddings_index[w] = np.append(model_ug_cbow.wv[w],model_ug_sg.wv[w])\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer #to split the words in a sentence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer = Tokenizer(num_words=100000)#take only the 100k most frequent words\n",
    "tokenizer.fit_on_texts(x_train)\n",
    "sequences = tokenizer.texts_to_sequences(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#figure out the max number of words in one sentence\n",
    "length = []\n",
    "for x in x_train:\n",
    "    length.append(len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1423"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(length) #check the length we need to assign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of tensor is (22500, 1425)\n"
     ]
    }
   ],
   "source": [
    "x_train_seq = pad_sequences(sequences,maxlen=1425)\n",
    "print('Shape of tensor is',x_train_seq.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences_val = tokenizer.texts_to_sequences(x_val)\n",
    "x_val_seq = pad_sequences(sequences_val,maxlen=1425)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = 100000 #limitation of 100k most frequent words\n",
    "embedding_matrix = np.zeros((num_words, 200))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if i >= num_words:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this CNN model, a simple one with one 1D conv layer and a functional API model were trained and compared. Second model performed sightly better than the simple one, so we will go with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding,Dense,Dropout,Activation,Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Naomi\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\Naomi\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From C:\\Users\\Naomi\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From C:\\Users\\Naomi\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Train on 22500 samples, validate on 2500 samples\n",
      "Epoch 1/5\n",
      " - 610s - loss: 0.3599 - acc: 0.8378 - val_loss: 0.3329 - val_acc: 0.8556\n",
      "Epoch 2/5\n",
      " - 605s - loss: 0.1632 - acc: 0.9386 - val_loss: 0.2804 - val_acc: 0.8928\n",
      "Epoch 3/5\n",
      " - 603s - loss: 0.0425 - acc: 0.9868 - val_loss: 0.3241 - val_acc: 0.8948\n",
      "Epoch 4/5\n",
      " - 600s - loss: 0.0124 - acc: 0.9964 - val_loss: 0.4175 - val_acc: 0.9008\n",
      "Epoch 5/5\n",
      " - 602s - loss: 0.0070 - acc: 0.9977 - val_loss: 0.4581 - val_acc: 0.8944\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1bb9efc0cc0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = Input(shape=(1425,),dtype='int32')\n",
    "encoder = Embedding(100000, 200, weights=[embedding_matrix], input_length=1425, trainable=True)(data)\n",
    "\n",
    "layer1 = Sequential() #bigram\n",
    "layer1=Conv1D(filters=100, kernel_size=2, padding='valid', activation='relu', strides=1)(encoder)\n",
    "layer1=GlobalMaxPooling1D()(layer1)\n",
    "\n",
    "layer2 = Sequential() #trigram\n",
    "layer2=Conv1D(filters=100, kernel_size=3, padding='valid', activation='relu', strides=1)(encoder)\n",
    "layer2=GlobalMaxPooling1D()(layer2)\n",
    "\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import concatenate\n",
    "merged = concatenate([layer1, layer2], axis=1)\n",
    "merged = Dense(256, activation='relu')(merged)\n",
    "merged = Dropout(0.2)(merged)\n",
    "merged = Dense(1)(merged)\n",
    "output = Activation('sigmoid')(merged)\n",
    "model = Model(inputs=[data], outputs=[output])\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(x_train_seq, y_train, validation_data=(x_val_seq, y_val), epochs=5, batch_size=32, verbose=2)\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Simple CNN\n",
    "cnn = Sequential()\n",
    "e = Embedding(100000, 200, weights=[embedding_matrix], input_length=1425, trainable=True)\n",
    "cnn.add(e)\n",
    "cnn.add(Conv1D(filters=100, kernel_size=2, padding='valid', activation='relu', strides=1))\n",
    "cnn.add(GlobalMaxPooling1D())\n",
    "cnn.add(Dropout(0.2))\n",
    "cnn.add(Dense(256, activation='relu'))\n",
    "cnn.add(Dense(1, activation='sigmoid'))\n",
    "cnn.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "cnn.fit(x_train_seq, y_train, validation_data=(x_val_seq, y_val), epochs=5, batch_size=32, verbose=2)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences_test = tokenizer.texts_to_sequences(x_test)\n",
    "x_test_seq = pad_sequences(sequences_test, maxlen=1425)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.87001638, 0.88927194]),\n",
       " array([0.89208, 0.86672]),\n",
       " array([0.88091006, 0.87785115]),\n",
       " array([12500, 12500], dtype=int64))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model.evaluate(x=x_test_seq, y=y_test)\n",
    "y_pred = model.predict(x_test_seq)\n",
    "prediction=[]\n",
    "for i in range(25000):\n",
    "    if (y_pred[i]<0.5):\n",
    "        prediction.append('0')\n",
    "    else:\n",
    "        prediction.append('1')\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "precision_recall_fscore_support(y_test, prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 59s - ETA: 59 - ETA: 59 - ETA: 59 - ETA: 59 - ETA: 58 - ETA: 58 - ETA: 58 - ETA: 58 - ETA: 58 - ETA: 57 - ETA: 57 - ETA: 57 - ETA: 57 - ETA: 57 - ETA: 56 - ETA: 56 - ETA: 56 - ETA: 56 - ETA: 56 - ETA: 55 - ETA: 55 - ETA: 55 - ETA: 55 - ETA: 55 - ETA: 54 - ETA: 54 - ETA: 54 - ETA: 54 - ETA: 54 - ETA: 53 - ETA: 53 - ETA: 53 - ETA: 53 - ETA: 53 - ETA: 52 - ETA: 52 - ETA: 52 - ETA: 52 - ETA: 52 - ETA: 51 - ETA: 51 - ETA: 51 - ETA: 51 - ETA: 51 - ETA: 50 - ETA: 50 - ETA: 50 - ETA: 50 - ETA: 50 - ETA: 49 - ETA: 49 - ETA: 49 - ETA: 49 - ETA: 49 - ETA: 48 - ETA: 48 - ETA: 48 - ETA: 48 - ETA: 48 - ETA: 48 - ETA: 47 - ETA: 47 - ETA: 47 - ETA: 47 - ETA: 47 - ETA: 46 - ETA: 46 - ETA: 46 - ETA: 46 - ETA: 46 - ETA: 45 - ETA: 45 - ETA: 45 - ETA: 45 - ETA: 45 - ETA: 44 - ETA: 44 - ETA: 44 - ETA: 44 - ETA: 44 - ETA: 43 - ETA: 43 - ETA: 43 - ETA: 43 - ETA: 43 - ETA: 42 - ETA: 42 - ETA: 42 - ETA: 42 - ETA: 42 - ETA: 41 - ETA: 41 - ETA: 41 - ETA: 41 - ETA: 41 - ETA: 40 - ETA: 40 - ETA: 40 - ETA: 40 - ETA: 40 - ETA: 39 - ETA: 39 - ETA: 39 - ETA: 39 - ETA: 39 - ETA: 38 - ETA: 38 - ETA: 38 - ETA: 38 - ETA: 38 - ETA: 37 - ETA: 37 - ETA: 37 - ETA: 37 - ETA: 37 - ETA: 36 - ETA: 36 - ETA: 36 - ETA: 36 - ETA: 36 - ETA: 35 - ETA: 35 - ETA: 35 - ETA: 35 - ETA: 35 - ETA: 34 - ETA: 34 - ETA: 34 - ETA: 34 - ETA: 34 - ETA: 34 - ETA: 33 - ETA: 33 - ETA: 33 - ETA: 33 - ETA: 33 - ETA: 32 - ETA: 32 - ETA: 32 - ETA: 32 - ETA: 32 - ETA: 31 - ETA: 31 - ETA: 31 - ETA: 31 - ETA: 31 - ETA: 30 - ETA: 30 - ETA: 30 - ETA: 30 - ETA: 30 - ETA: 29 - ETA: 29 - ETA: 29 - ETA: 29 - ETA: 29 - ETA: 28 - ETA: 28 - ETA: 28 - ETA: 28 - ETA: 28 - ETA: 27 - ETA: 27 - ETA: 27 - ETA: 27 - ETA: 27 - ETA: 26 - ETA: 26 - ETA: 26 - ETA: 26 - ETA: 26 - ETA: 25 - ETA: 25 - ETA: 25 - ETA: 25 - ETA: 25 - ETA: 24 - ETA: 24 - ETA: 24 - ETA: 24 - ETA: 24 - ETA: 23 - ETA: 23 - ETA: 23 - ETA: 23 - ETA: 23 - ETA: 22 - ETA: 22 - ETA: 22 - ETA: 22 - ETA: 22 - ETA: 21 - ETA: 21 - ETA: 21 - ETA: 21 - ETA: 21 - ETA: 20 - ETA: 20 - ETA: 20 - ETA: 20 - ETA: 20 - ETA: 19 - ETA: 19 - ETA: 19 - ETA: 19 - ETA: 19 - ETA: 18 - ETA: 18 - ETA: 18 - ETA: 18 - ETA: 18 - ETA: 17 - ETA: 17 - ETA: 17 - ETA: 17 - ETA: 17 - ETA: 16 - ETA: 16 - ETA: 16 - ETA: 16 - ETA: 16 - ETA: 15 - ETA: 15 - ETA: 15 - ETA: 15 - ETA: 15 - ETA: 14 - ETA: 14 - ETA: 14 - ETA: 14 - ETA: 14 - ETA: 13 - ETA: 13 - ETA: 13 - ETA: 13 - ETA: 13 - ETA: 12 - ETA: 12 - ETA: 12 - ETA: 12 - ETA: 12 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 11 - ETA: 10 - ETA: 10 - ETA: 10 - ETA: 10 - ETA: 10 - ETA: 9 - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - ETA:  - 155s 6ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.542779450494945, 0.8794]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x=x_test_seq, y=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Import - https://github.com/SrinidhiRaghavan/AI-Sentiment-Analysis-on-IMDB-Dataset/blob/master/driver_3.py\n",
    "\n",
    "Word2Vec Training - https://towardsdatascience.com/another-twitter-sentiment-analysis-with-python-part-10-neural-network-with-a6441269aa3c"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
